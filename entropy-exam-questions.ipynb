{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4b29f9",
   "metadata": {},
   "source": [
    "# [Machine Learning](https://github.com/marcocrowe/learn-machine-learning \"Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e4d1d",
   "metadata": {},
   "source": [
    "## Sample Question - Weather Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1d86d",
   "metadata": {},
   "source": [
    "The following dataset contains the descriptive features (`Humid`, `Cloudy`, `Windy`) which determine whether it will `Rain` (target feature). Given the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe6ac4",
   "metadata": {},
   "source": [
    "| #  | Humid | Cloudy | Windy | Rain |\n",
    "|----|-------|--------|-------|------|\n",
    "| 1  | True  | False  | True  | Yes  |\n",
    "| 2  | True  | True   | False | Yes  |\n",
    "| 3  | True  | True   | False | Yes  |\n",
    "| 4  | False | True   | True  | No   |\n",
    "| 5  | False | False  | False | No   |\n",
    "| 6  | False | False  | False | No   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7eeef8",
   "metadata": {},
   "source": [
    "### Part 1 - Discuss what is meant by Entropy. [7 Marks]\n",
    "\n",
    "Use both the definition and formula for Entropy to clarify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b441741",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Entropy is a measure of randomness or uncertainty in a dataset. In the context of decision trees and classification problems, entropy is used to quantify the impurity of a collection of examples. A dataset with high entropy has a lot of disorder, meaning the classes are distributed randomly or evenly. On the other hand, a dataset with low entropy has less disorder, indicating that the examples belong predominantly to one class.\n",
    "\n",
    "In lay terms entropy is the 'sameness' or homogeneity in a dataset measured between 0 and 1. If the entropy is 0, it means that the dataset is perfectly homogenous, and all examples belong are the 'same'. If the entropy is 1, it means that the dataset is completely random, and the examples are not the 'same'.\n",
    "\n",
    "The formula for entropy is given by:\n",
    "\n",
    "$$ \\text{Entropy}(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) $$\n",
    "\n",
    "where:\n",
    "- $S$ is the dataset\n",
    "- the classes of the target feature `Rain` are `Yes` and `No`\n",
    "- $i$ is the index of the class. i.e. $i=1$ represents the first class is `Yes` and $i=2$ represents the second class is `No`\n",
    "- $p_i$ is the proportion of examples in class $i$ in the dataset\n",
    "- $c$ is the number of classes in the feature/target variable of interest.\n",
    "- $\\sum_{i=1}^{c}$ is the sum of the entropy for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8584f",
   "metadata": {},
   "source": [
    "### Part 2 - Calculate the Entropy [6 Marks]\n",
    "\n",
    "Calculate the Entropy for the entire dataset above using the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03079c",
   "metadata": {},
   "source": [
    "\n",
    "$S =6$ (total number of examples)  \n",
    "\n",
    "The classes for the target feature `Rain` are `Yes` and `No`.  The number of classes $c = 2$\n",
    "\n",
    "The sum of `Yes` is 3 and the sum of `No` is 3.\n",
    "\n",
    "$P(i=1) = P( \\text{Rain} = Yes) = \\frac{3}{6} = 0.5$  \n",
    "$P(i=2) = P( \\text{Rain} = No) = \\frac{3}{6} = 0.5$\n",
    "\n",
    "$ \\text{Entropy}(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) $\n",
    "\n",
    "since $c = 2$\n",
    " \n",
    "$ \\text{Entropy}(S) = - (p_1 \\log_2(p_1) + p_2 \\log_2(p_2)) $ \n",
    "\n",
    "Plug in the values into the formula:  \n",
    "\n",
    "$ \\text{Entropy}(S) = - (0.5 \\log_2(0.5) + 0.5 \\log_2(0.5)) $\n",
    "\n",
    "$ \\text{Entropy}(S) = - (0.5 \\times -1 + 0.5 \\times -1) $\n",
    "\n",
    "$ \\text{Entropy}(S) = - (-0.5 + -0.5) $\n",
    "\n",
    "$ \\text{Entropy}(S) = - (-1) $\n",
    "\n",
    "$ \\text{Entropy}(S) = 1 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed32c4d",
   "metadata": {},
   "source": [
    "### Alt Sample Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b54324",
   "metadata": {},
   "source": [
    "\n",
    "Calculate the Entropy for the entire dataset above using the Windy feature.\n",
    "\n",
    "$S =6$ (total number of examples)\n",
    "\n",
    "The classes for the target feature `Windy` are `True` and `False`.  The number of classes $c = 2$\n",
    "\n",
    "The sum of `True` is 2 and the sum of `False` is 4.\n",
    "\n",
    "$P(1) = P(Windy = True) = \\frac{2}{6} = 0.33$  \n",
    "$P(2) = P(Windy = False) = \\frac{4}{6} = 0.67$  \n",
    "\n",
    "$Entropy(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
    "\n",
    "Plug in the values:\n",
    "\n",
    "$Entropy(S) = - \\left( \\frac{2}{6} \\log_2(\\frac{2}{6}) + \\frac{4}{6} \\log_2(\\frac{4}{6}) \\right) \\\\ = (0.33 \\times -1.58) + (0.67 \\times -0.58) $\n",
    "$= 0.92$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263e7ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b49ee5ed",
   "metadata": {},
   "source": [
    "$\\cdot\\frac{\\ln\\left(.33\\right)}{\\ln\\left(2\\right)}+0.67\\cdot\\frac{\\ln\\left(.67\\right)}{\\ln\\left(2\\right)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126a9e1",
   "metadata": {},
   "source": [
    "### Part 3 - Calculate the information gain [12 Marks]\n",
    "\n",
    "Demonstrate how you would calculate the information gain for each of the above features (`Humid`, `Cloudy`, `Windy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268ecff",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Information gain measures the effectiveness of a feature in classifying the dataset. It indicates how much entropy is reduced when a dataset is split on a particular feature.\n",
    "\n",
    "To calculate information gain for a feature, we first calculate the weighted average of the entropies of the resulting subsets after splitting the dataset on that feature\n",
    "\n",
    "Our feature set is `Humid`, `Cloudy`, and `Windy`. We will calculate the information gain for each feature.\n",
    "\n",
    "The formula for information gain is given by:\n",
    "\n",
    "$$ Information Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v) $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $S$ is the dataset\n",
    "- $A$ is the feature\n",
    "- $Values(A)$ is the set of possible values of feature $A$\n",
    "- $S_v$ is the subset of $S$ for which feature $A$ has value $v$\n",
    "- $|S|$ is the total number of examples in $S$\n",
    "- $|S_v|$ is the number of examples in $S_v$\n",
    "- $Entropy(S)$ is the entropy of the dataset $S$\n",
    "- $Entropy(S_v)$ is the entropy of the subset $S_v$\n",
    "- $Information Gain(S, A)$ is the information gain of feature $A$ on dataset $S$\n",
    "- $v$ is a value of feature $A$\n",
    "\n",
    "Let's calculate the information gain for each feature:\n",
    "\n",
    "1. **Humid**:\n",
    "\n",
    "- Split the dataset based on the `Humid` feature, we have two classes `True` and `False`, $c = 2$\n",
    "  - $P(1) = P(Humid = True) = \\frac{3}{6} = 0.5$  \n",
    "  - $P(2) = P(Humid = False) = \\frac{3}{6} = 0.5$  \n",
    "- Calculate the entropy of the subsets:\n",
    "  - For `Humid = True`:\n",
    "      - Number of examples = 3\n",
    "      - Number of `Yes` = 3\n",
    "      - Number of `No` = 0\n",
    "      - $Entropy(S_{Humid=True}) = 0$ (since all examples are of the same class)\n",
    "  - For `Humid = False`:\n",
    "      - Number of examples = 3\n",
    "      - Number of `Yes` = 0\n",
    "      - Number of `No` = 3\n",
    "      - $Entropy(S_{Humid=False}) = 0$ (since all examples are of the same class)\n",
    "- Calculate the information gain:\n",
    "  - $ Information Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v) $\n",
    "  - $Information Gain(S, Humid) = Entropy(S) -  \\frac{3}{6} \\times 0 + \\frac{3}{6} \\times 0  = 1 - 0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e110921",
   "metadata": {},
   "source": [
    "## Lab Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f154178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame, read_csv\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8a7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7d347b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Annual Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>never married</td>\n",
       "      <td>transport</td>\n",
       "      <td>25K-50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>married</td>\n",
       "      <td>professional</td>\n",
       "      <td>25K-50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>high school</td>\n",
       "      <td>never married</td>\n",
       "      <td>agriculture</td>\n",
       "      <td>&lt;25K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>married</td>\n",
       "      <td>professional</td>\n",
       "      <td>25K-50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>high school</td>\n",
       "      <td>married</td>\n",
       "      <td>agriculture</td>\n",
       "      <td>25K-50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>high school</td>\n",
       "      <td>never married</td>\n",
       "      <td>armed forces</td>\n",
       "      <td>&lt;25K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52</td>\n",
       "      <td>high school</td>\n",
       "      <td>divorced</td>\n",
       "      <td>transport</td>\n",
       "      <td>25K-50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>doctorate</td>\n",
       "      <td>married</td>\n",
       "      <td>professional</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age    Education Marital Status    Occupation Annual Income\n",
       "0   39    bachelors  never married     transport       25K-50K\n",
       "1   50    bachelors        married  professional       25K-50K\n",
       "2   18  high school  never married   agriculture          <25K\n",
       "3   28    bachelors        married  professional       25K-50K\n",
       "4   37  high school        married   agriculture       25K-50K\n",
       "5   24  high school  never married  armed forces          <25K\n",
       "6   52  high school       divorced     transport       25K-50K\n",
       "7   40    doctorate        married  professional          >50K"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa402b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'bachelors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44172\\1914698700.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdecisionTreeClassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"entropy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdecisionTreeClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 )\n\u001b[0;32m   1150\u001b[0m             ):\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    955\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \"\"\"\n\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m         super()._fit(\n\u001b[0m\u001b[0;32m    960\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    238\u001b[0m             check_X_params = dict(\n\u001b[0;32m    239\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             )\n\u001b[0;32m    241\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    243\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             )\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[1;31m# :(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_X_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                     \u001b[0mcheck_X_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    914\u001b[0m                         )\n\u001b[0;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m                 raise ValueError(\n\u001b[0;32m    920\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         if (\n\u001b[0;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'bachelors'"
     ]
    }
   ],
   "source": [
    "features = dataframe.columns[0:-1]\n",
    "target = dataframe.columns[-1]\n",
    "decisionTreeClassifier = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "decisionTreeClassifier.fit(dataframe[features], dataframe[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f6564",
   "metadata": {},
   "source": [
    "\n",
    "# Information Based Learning - ID3 Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c452d71",
   "metadata": {},
   "source": [
    "\n",
    "The dataset below describes the predictive annual income of individuals based on the descriptive features `Age`, `Education`, `Marital Status` and `Occupation`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caccebea",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1\n",
    "\n",
    "Calculate the entropy for the entire dataset. The `Annual Income` is the target feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb191d4",
   "metadata": {},
   "source": [
    "\n",
    "### Answer 1\n",
    "\n",
    "The entropy of the entire dataset is calculated as follows:\n",
    "\n",
    "$$ \\text{Entropy(\\text{Annual Income})} = - \\sum_{i=1}^{n} p_i \\log_2 p_i $$\n",
    "\n",
    "where $p_i$ is the probability of the $i$th class.\n",
    "\n",
    "The probability of each class of `Annual Income` is calculated as follows:\n",
    "\n",
    "$p(\\text{<25K}) = \\frac{2}{8} = 0.25$  \n",
    "$p(\\text{25K-50K}) = \\frac{5}{8} = 0.625$  \n",
    "$p(\\text{>50K}) = \\frac{1}{8} = 0.125$  \n",
    "\n",
    "The entropy of the entire dataset is calculated as follows:\n",
    "\n",
    "$\\text{Entropy(\\text{AI})} = - (\\text{Entropy(\\text{AI=<25K})} + \\text{Entropy(\\text{AI=25K-50K})} + \\text{Entropy(\\text{AI=>50K})})$  \n",
    "$\\text{Entropy(\\text{AI})} = - ((0.25 \\log_2 0.25) + (0.625 \\log_2 0.625) + (0.125 \\log_2 0.125))$  \n",
    "$\\text{Entropy(\\text{AI})} = - ((0.25 \\times -2) + (0.625 \\times 0.6781) + (0.125 \\times -3))$  \n",
    "$\\text{Entropy(\\text{AI})} = - (-0.5 + -0.4238 + -0.375)$  \n",
    "$\\text{Entropy(\\text{AI})} = - (-1.2988)$  \n",
    "$\\text{Entropy(\\text{AI})} = 1.2988$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124be69",
   "metadata": {},
   "source": [
    "\n",
    "| Id | Age | Education   | Marital Status | Occupation   | Annual Income |\n",
    "|----|-----|-------------|----------------|--------------|---------------|\n",
    "| 1  | 39  | bachelors   | never married  | transport    | 25K-50K       |\n",
    "| 2  | 50  | bachelors   | married        | professional | 25K-50K       |\n",
    "| 3  | 18  | high school | never married  | agriculture  | <25K          |\n",
    "| 4  | 28  | bachelors   | married        | professional | 25K-50K       |\n",
    "| 5  | 37  | high school | married        | agriculture  | 25K-50K       |\n",
    "| 6  | 24  | high school | never married  | armed forces | <25K          |\n",
    "| 7  | 52  | high school | divorced       | transport    | 25K-50K       |\n",
    "| 8  | 40  | doctorate   | married        | professional | >50K          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23ec4a",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ce610",
   "metadata": {},
   "source": [
    "\n",
    "Using this dataset construct the decision tree that would be generated by the ID3 algorithm: using entropy-based information gain. (only use the `Education` `Marital Status`, `Occupation` descriptive features)\n",
    "\n",
    "Clearly show the entropy and information gain for each feature that was generated by the ID3 algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d2d4a",
   "metadata": {},
   "source": [
    "\n",
    "### Answer 2\n",
    "\n",
    "The decision tree generated by the ID3 algorithm is as follows:\n",
    "\n",
    "1. **Root Node**: The root node is the feature with the highest information gain.  \n",
    "   - Calculate the information gain for each feature: `Education`, `Marital Status`, and `Occupation`.\n",
    "\n",
    "$$ \\text{Information Gain} = \\text{Entropy(\\text{Annual Income})} - \\text{Entropy(\\text{Annual Income} | \\text{Feature})} $$\n",
    "\n",
    "   - Calculate the entropy for each feature.\n",
    " - **Education**:\n",
    "   - Calculate the entropy for each class of `Education`.\n",
    "   - Calculate the information gain for `Education`.\n",
    "   - $p(\\text{bachelors}) = \\frac{3}{8} = 0.375$\n",
    "   - $p(\\text{high school}) = \\frac{4}{8} = 0.5$\n",
    "   - $p(\\text{doctorate}) = \\frac{1}{8} = 0.125$\n",
    "   - $ \\text{Entropy(\\text{AI | Education})} = - \\sum_{i=1}^{n} p_i \\text{Entropy(\\text{AI=class})} $\n",
    "   - $ \\text{Entropy(\\text{AI | Education})} = -(\\text{Entropy(\\text{AI=bachelors})} + \\text{Entropy(\\text{AI=high school})} + \\text{Entropy(\\text{AI=doctorate})})$\n",
    "   - $\\text{Entropy(\\text{AI | Education})} = - (0.375 \\times log_2 0.375) + (0.5 \\times log_2 0.5) + (0.125 \\times log_2 0.125)$\n",
    "   - $\\text{Entropy(\\text{AI | Education})} = - (0.375 \\times -1.415) + (0.5 \\times -1) + (0.125 \\times -3)$\n",
    "   - $\\text{Entropy(\\text{AI | Education})} = - (-0.5306 - 0.5 - 0.375)$  \n",
    "   - $\\text{Entropy(\\text{AI | Education})} = -(-1.4056) = 1.4056$  \n",
    "   - $S_{\\text{bachelors}} = 3$\n",
    "   - $S_{\\text{high school}} = 4$\n",
    "   - $S_{\\text{doctorate}} = 1$\n",
    "\n",
    "\n",
    "$ \\text{Information Gain} = \\text{Entropy(\\text{Annual Income})} - \\sum_{i=1}^{n} \\left( \\frac{|S_i|}{|S|} \\times \\text{Entropy}(S_i) \\right) $\n",
    "\n",
    "\n",
    "## Question 3\n",
    "\n",
    "A colleague suggests that the feature `Marital Status` should be the root of the tree. Would you agree with this? Clearly explain your reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1be5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Copyright &copy; 2024 Mark Crowe <https://github.com/marcocrowe>. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
