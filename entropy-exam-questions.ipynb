{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4b29f9",
   "metadata": {},
   "source": [
    "# [Machine Learning](https://github.com/marcocrowe/learn-machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e4d1d",
   "metadata": {},
   "source": [
    "## Sample Question - Weather Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1d86d",
   "metadata": {},
   "source": [
    "The following dataset contains the descriptive features (`Humid`, `Cloudy`, `Windy`) which determine whether it will `Rain` (target feature). Given the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe6ac4",
   "metadata": {},
   "source": [
    "| #  | Humid | Cloudy | Windy | Rain |\n",
    "|----|-------|--------|-------|------|\n",
    "| 1  | True  | False  | True  | Yes  |\n",
    "| 2  | True  | True   | False | Yes  |\n",
    "| 3  | True  | True   | False | Yes  |\n",
    "| 4  | False | True   | True  | No   |\n",
    "| 5  | False | False  | False | No   |\n",
    "| 6  | False | False  | False | No   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7eeef8",
   "metadata": {},
   "source": [
    "### Part 1 - Discuss what is meant by Entropy. [7 Marks]\n",
    "\n",
    "Use both the definition and formula for Entropy to clarify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b441741",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Entropy is a measure of randomness or uncertainty in a dataset. In the context of decision trees and classification problems, entropy is used to quantify the impurity of a collection of examples. A dataset with high entropy has a lot of disorder, meaning the classes are distributed randomly or evenly. On the other hand, a dataset with low entropy has less disorder, indicating that the examples belong predominantly to one class.\n",
    "\n",
    "In lay terms entropy is the 'sameness' or homogeneity in a dataset measured between 0 and 1. If the entropy is 0, it means that the dataset is perfectly homogenous, and all examples belong are teh 'same'. If the entropy is 1, it means that the dataset is completely random, and the examples are not the 'same'.\n",
    "\n",
    "The formula for entropy is given by:\n",
    "\n",
    "$$ \\text{Entropy}(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) $$\n",
    "\n",
    "where:\n",
    "- $S$ is the dataset\n",
    "- $c$ is the number of classes in the dataset\n",
    "- $\\sum_{i=1}^{c}$ is the sum of the entropy for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8584f",
   "metadata": {},
   "source": [
    "### Part 2 - Calculate the Entropy [6 Marks]\n",
    "\n",
    "Calculate the Entropy for the entire dataset above using the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03079c",
   "metadata": {},
   "source": [
    "\n",
    "$S =6$ (total number of examples)  \n",
    "\n",
    "The classes for the target feature `Rain` are `Yes` and `No`.  The number of classes $c = 2$\n",
    "\n",
    "The sum of `Yes` is 3 and the sum of `No` is 3.\n",
    "\n",
    "$P(1) = P(Rain = Yes) = \\frac{3}{6} = 0.5$  \n",
    "$P(2) = P(Rain = No) = \\frac{3}{6} = 0.5$\n",
    "\n",
    "$ Entropy(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i) $\n",
    "\n",
    "$ Entropy(S) = - (p_1 \\log_2(p_1) + p_2 \\log_2(p_2)) $ (since $c = 2$)  \n",
    "\n",
    "Plug in the values into the formula:  \n",
    "\n",
    "$ Entropy(S) = - (0.5 \\log_2(0.5) + 0.5 \\log_2(0.5)) $\n",
    "\n",
    "$ Entropy(S) = - (0.5 \\times -1 + 0.5 \\times -1) $\n",
    "\n",
    "$ Entropy(S) = - (-0.5 + -0.5) $\n",
    "\n",
    "$ Entropy(S) = - (-1) $\n",
    "\n",
    "$ Entropy(S) = 1 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed32c4d",
   "metadata": {},
   "source": [
    "### Alt Sample Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b54324",
   "metadata": {},
   "source": [
    "\n",
    "Calculate the Entropy for the entire dataset above using the Windy feature.\n",
    "\n",
    "$S =6$ (total number of examples)\n",
    "\n",
    "The classes for the target feature `Windy` are `True` and `False`.  The number of classes $c = 2$\n",
    "\n",
    "The sum of `True` is 2 and the sum of `False` is 4.\n",
    "\n",
    "$P(1) = P(Windy = True) = \\frac{2}{6} = 0.33$  \n",
    "$P(2) = P(Windy = False) = \\frac{4}{6} = 0.67$  \n",
    "\n",
    "$Entropy(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
    "\n",
    "Plug in the values:\n",
    "\n",
    "$Entropy(S) = - \\left( \\frac{2}{6} \\log_2(\\frac{2}{6}) + \\frac{4}{6} \\log_2(\\frac{4}{6}) \\right) \\\\ = (0.33 \\times -1.58) + (0.67 \\times -0.58) $\n",
    "$= 0.92$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263e7ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b49ee5ed",
   "metadata": {},
   "source": [
    "$\\cdot\\frac{\\ln\\left(.33\\right)}{\\ln\\left(2\\right)}+0.67\\cdot\\frac{\\ln\\left(.67\\right)}{\\ln\\left(2\\right)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126a9e1",
   "metadata": {},
   "source": [
    "### Part 3 - Calculate the information gain [12 Marks]\n",
    "\n",
    "Demonstrate how you would calculate the information gain for each of the above features (`Humid`, `Cloudy`, `Windy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268ecff",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Information gain measures the effectiveness of a feature in classifying the dataset. It indicates how much entropy is reduced when a dataset is split on a particular feature.\n",
    "\n",
    "To calculate information gain for a feature, we first calculate the weighted average of the entropies of the resulting subsets after splitting the dataset on that feature\n",
    "\n",
    "Our feature set is `Humid`, `Cloudy`, and `Windy`. We will calculate the information gain for each feature.\n",
    "\n",
    "The formula for information gain is given by:\n",
    "\n",
    "$$ Information Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v) $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $S$ is the dataset\n",
    "- $A$ is the feature\n",
    "- $Values(A)$ is the set of possible values of feature $A$\n",
    "- $S_v$ is the subset of $S$ for which feature $A$ has value $v$\n",
    "- $|S|$ is the total number of examples in $S$\n",
    "- $|S_v|$ is the number of examples in $S_v$\n",
    "- $Entropy(S)$ is the entropy of the dataset $S$\n",
    "- $Entropy(S_v)$ is the entropy of the subset $S_v$\n",
    "- $Information Gain(S, A)$ is the information gain of feature $A$ on dataset $S$\n",
    "- $v$ is a value of feature $A$\n",
    "\n",
    "Let's calculate the information gain for each feature:\n",
    "\n",
    "1. **Humid**:\n",
    "\n",
    "- Split the dataset based on the `Humid` feature, we have two classes `True` and `False`, $c = 2$\n",
    "  - $P(1) = P(Humid = True) = \\frac{3}{6} = 0.5$  \n",
    "  - $P(2) = P(Humid = False) = \\frac{3}{6} = 0.5$  \n",
    "- Calculate the entropy of the subsets:\n",
    "  - For `Humid = True`:\n",
    "      - Number of examples = 3\n",
    "      - Number of `Yes` = 3\n",
    "      - Number of `No` = 0\n",
    "      - $Entropy(S_{Humid=True}) = 0$ (since all examples are of the same class)\n",
    "  - For `Humid = False`:\n",
    "      - Number of examples = 3\n",
    "      - Number of `Yes` = 0\n",
    "      - Number of `No` = 3\n",
    "      - $Entropy(S_{Humid=False}) = 0$ (since all examples are of the same class)\n",
    "- Calculate the information gain:\n",
    "  - $ Information Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v) $\n",
    "  - $Information Gain(S, Humid) = Entropy(S) -  \\frac{3}{6} \\times 0 + \\frac{3}{6} \\times 0  = 1 - 0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1be5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Copyright &copy; 2024 Mark Crowe <https://github.com/marcocrowe>. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
