
# Information Based Learning - ID3 Algorithm

The dataset below describes the predictive annual income of individuals based on the descriptive features `Age`, `Education`, `Marital Status` and `Occupation`.

| Id | Age | Education   | Marital Status | Occupation   | Annual Income |
|----|-----|-------------|----------------|--------------|---------------|
| 1  | 39  | bachelors   | never married  | transport    | 25K-50K       |
| 2  | 50  | bachelors   | married        | professional | 25K-50K       |
| 3  | 18  | high school | never married  | agriculture  | <25K          |
| 4  | 28  | bachelors   | married        | professional | 25K-50K       |
| 5  | 37  | high school | married        | agriculture  | 25K-50K       |
| 6  | 24  | high school | never married  | armed forces | <25K          |
| 7  | 52  | high school | divorced       | transport    | 25K-50K       |
| 8  | 40  | doctorate   | married        | professional | >50K          |

## Question 1

Calculate the entropy for the entire dataset. The `Annual Income` is the target feature.

### Answer 1

The entropy of the entire dataset is calculated as follows:

$$Entropy({Annual Income}) = - \sum_{i=1}^{n} p_i \log_2 p_i$$

where $p_i$ is the probability of the $i$th class.

The probability of each class of `Annual Income` is calculated as follows:

$p(<25K) = \frac{2}{8} = 0.25$  
$p(25K-50K) = \frac{5}{8} = 0.625$  
$p(>50K) = \frac{1}{8} = 0.125$  

The entropy of the entire dataset is calculated as follows:

$H(AI) = - (H(AI=<25K) + H(AI=25K-50K) + H(AI=>50K))$  
$H(AI) = - ((0.25 \log_2 0.25) + (0.625 \log_2 0.625) + (0.125 \log_2 0.125))$  
$H(AI) = - ((0.25 \times -2) + (0.625 \times 0.6781) + (0.125 \times -3))$  
$H(AI) = - (-0.5 + -0.4238 + -0.375)$  
$H(AI) = - (-1.2988)$  
$H(AI) = 1.2988$

## Question 2

Using this dataset construct the decision tree that would be generated by the ID3 algorithm: using entropy-based information gain. (only use the `Education` `Marital Status`, `Occupation` descriptive features)

Clearly show the entropy and information gain for each feature that was generated by the ID3 algorithm.

### Answer 2

$IG(E) = H(D) - \sum_{j} \frac{|D_j|}{|D|} \cdot H(D_j)$

---

Calculate the weighted average entropy for `Education`, E.

$H(E, D) = \frac{|E_{\text{bachelors}}|}{|D|} H(E_{\text{bachelors}}) + \frac{|E_{\text{highschool}}|}{|D|} H(E_{\text{highschool}})+ \frac{|E_{\text{doctorate}}|}{|D|} H(E_{\text{doctorate}})$  
$H(E, D) = \frac{3}{8} H(E_{\text{bachelors}}) + \frac{4}{8} H(E_{\text{highschool}}) + \frac{1}{8} H(E_{\text{doctorate}})$  
$H(E_{\text{bachelors}}) = ((\frac{3}{3} \log_2 \frac{3}{3}) + (\frac{0}{3} \times \log_2 \frac{0}{3})+ (\frac{0}{3} \times \log_2 \frac{0}{3})) = 0$
$H(E_{\text{highschool}}) = ((\frac{2}{4} \log_2 \frac{2}{4}) + (\frac{2}{4} \times \log_2 \frac{2}{4})+ (\frac{0}{4} \times \log_2 \frac{0}{4})) = -0.5 - 0.5 + 0 = -1$
$H(E_{\text{doctorate}}) = ((\frac{1}{1} \log_2 \frac{1}{1}) + (\frac{0}{1} \times \log_2 \frac{0}{1})+ (\frac{0}{1} \times \log_2 \frac{0}{1})) = 0$

$H(E, D) = \frac{3}{8} \times 0 + \frac{4}{8} \times -1 + \frac{1}{8} \times 0$
$H(E, D) = 0 - 0.5 + 0 = -0.5$
$I(Education) = 1.2988 - (-0.5) = 1.7988$

---

Calculate the weighted average entropy for `Marital Status`, MS.

$H(MS, D) = \frac{|MS_{\text{never married}}|}{|D|} H(MS_{\text{never married}}) + \frac{|MS_{\text{married}}|}{|D|} H(MS_{\text{married}})+ \frac{|MS_{\text{divorced}}|}{|D|} H(MS_{\text{divorced}})$
$H(MS, D) = \frac{3}{8} H(MS_{\text{never married}}) + \frac{4}{8} H(MS_{\text{married}}) + \frac{1}{8} H(MS_{\text{divorced}})$  
$H(MS_{\text{never married}}) = ((\frac{2}{3} \log_2 \frac{2}{3}) + (\frac{1}{3} \times \log_2 \frac{1}{3})+ (\frac{0}{3} \times \log_2 \frac{0}{3})) = -0.3899 - -0.5283 = -0.9182$  
$H(MS_{\text{married}}) = ((\frac{0}{4} \log_2 \frac{0}{4}) + (\frac{3}{4} \times \log_2 \frac{3}{4})+ (\frac{1}{4} \times \log_2 \frac{1}{4})) = -0.3113 -0.5 + 0 = -0.8113$  
$H(MS_{\text{divorced}}) = ((\frac{0}{1} \log_2 \frac{0}{1}) + (\frac{0}{1} \times \log_2 \frac{0}{1})+ (\frac{1}{1} \times \log_2 \frac{1}{1})) = 0$  
$H(MS, D) = \frac{3}{8} \times -0.9182 + \frac{4}{8} \times -0.8113 + \frac{1}{8} \times 0$  
$H(MS, D) = -0.3443 - 0.4056 + 0 =  = -0.7499$  
$IG(MS) = 1.2988 - (-0.7499) = 2.0487$

---

Calculate the weighted average entropy for `Occupation`, O.

$H(O, D) = \frac{|O_{\text{transport}}|}{|D|} H(O_{\text{transport}}) + \frac{|O_{\text{professional}}|}{|D|} H(O_{\text{professional}})+ \frac{|O_{\text{agriculture}}|}{|D|} H(O_{\text{agriculture}}) + \frac{|O_{\text{armed forces}}|}{|D|} H(O_{\text{armed forces}})$  

$H(O, D) = \frac{2}{8} H(O_{\text{transport}}) + \frac{3}{8} H(O_{\text{professional}}) + \frac{2}{8} H(O_{\text{agriculture}}) + \frac{1}{8} H(O_{\text{armed forces}})$  

$H(O_{\text{transport}}) = ((\frac{2}{2} \log_2 \frac{2}{2}) + (\frac{0}{2} \times \log_2 \frac{0}{2})+ (\frac{0}{2} \times \log_2 \frac{0}{2}) = 0$  
$H(O_{\text{transport}}) = 1*0 - 0 - 0 = 0  $

$H(O_{\text{professional}}) = ((\frac{1}{3} \log_2 \frac{1}{3}) + (\frac{2}{3} \times \log_2 \frac{2}{3})+ (\frac{0}{3} \times \log_2 \frac{0}{3}) = -0.5283 - 0.3899 + 0 = -0.9182$

$H(O_{\text{agriculture}}) = ((\frac{1}{2} \log_2 \frac{1}{2}) + (\frac{1}{2} \times \log_2 \frac{1}{2})+ (\frac{0}{2} \times \log_2 \frac{0}{2}) = -0.5 - 0.5 + 0 = -1$

$H(O_{\text{armed forces}}) = ((\frac{0}{1} \log_2 \frac{0}{1}) + (\frac{0}{1} \times \log_2 \frac{0}{1})+ (\frac{1}{1} \times \log_2 \frac{1}{1}) = 0 + 0 + (1\times0) =0$
$H(O, D) = (\frac{2}{8} \times 0) + (\frac{3}{8} \times -0.9182) + (\frac{2}{8} \times -1 )+ (\frac{1}{8} \times 0)$
$H(O, D) = 0 - 0.3443 - 0.25 + 0 = -0.5943$
$IG(O) = 1.2988 - (-0.5943) = 1.8931$

## Question 3

A colleague suggests that the feature `Marital Status` should be the root of the tree. Would you agree with this? Clearly explain your reasoning.

### Answer 3

The feature `Marital Status` should be the root of the tree because it has the highest information gain of 2.0487. This means that it provides the most information about the target variable `Annual Income` compared to the other features. Therefore, it is a good choice for the root of the decision tree as it will help in making the most accurate predictions.
