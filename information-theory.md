
# Information Theory

Information theory is a branch of applied mathematics that revolves around quantifying information.

## Entropy

Entropy is a measure of the uncertainty or randomness of a system. It is a measure of the average amount of information produced by a stochastic source of data.

> "Never ever judge a book by its cover... unless you're talking about entropy." - Mark Crowe (2024)

Information Gain
